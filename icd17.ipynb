{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a1fd84-04c7-4025-a50b-150a95b1e152",
   "metadata": {},
   "source": [
    "# Notebook ICD - 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e517e961-c76e-4bf5-a115-7f816df754b9",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd230f6-f7f7-4476-9510-de08a10ad083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab3690-420e-4ac4-bcb7-5db3fe4dce82",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3e6290-3f3a-4618-a827-2948f36ef6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daff3dc-d455-45b2-ab70-5f7a5320d9e0",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7cd08-e11f-45d1-af60-9724e8bcdb75",
   "metadata": {},
   "source": [
    "Gradient Boosting for classification.\n",
    "\n",
    "This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b6b3869-1a3e-4db9-a107-82514c342577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         0.93333333 0.93333333 0.86666667\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "0.96\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=10)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa0e51-987b-4e24-b331-e91220d167df",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c626aa14-ed62-4f14-aec4-8e37fecc2a4e",
   "metadata": {},
   "source": [
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting. If samples are drawn with replacement, then the method is known as Bagging. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches.\n",
    "\n",
    "The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1698ae5-04c3-4875-aa2f-293c0a4957d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         0.93333333 0.93333333 0.93333333\n",
      " 0.8        1.         1.         1.        ]\n",
      "0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "clf = BaggingClassifier(n_estimators=10)\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff473e2a-1afd-4122-9971-b44400034327",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef1763-f376-407b-96af-7dd572f08659",
   "metadata": {},
   "source": [
    "Stack of estimators with a final classifier.\n",
    "\n",
    "Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n",
    "\n",
    "Note that estimators_ are fitted on the full X while final_estimator_ is trained using cross-validated predictions of the base estimators using cross_val_predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c05f5a-4946-4877-b566-9c807cc7a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe92ed7-c4c6-4522-bb4c-4417e0d7d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data[:, 1:3], iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57eab172-b99d-4f63-8426-ba547bc1ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GaussianNB()\n",
    "clf3 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf4 = svm.SVC()\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15889a5-5a87-4bca-a85f-9879bdb67cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4],\n",
    "                            meta_classifier=lr,\n",
    "                            random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63821dc8-ad14-479b-a048-3fafaf4bfa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation:\n",
      "\n",
      "Accuracy: 0.92 (+/- 0.06) [Decision Tree]\n",
      "Accuracy: 0.91 (+/- 0.05) [Naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.05) [kNN]\n",
      "Accuracy: 0.95 (+/- 0.04) [SVM]\n",
      "Accuracy: 0.95 (+/- 0.05) [StackingClassifier]\n"
     ]
    }
   ],
   "source": [
    "print('10-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, sclf], \n",
    "                      ['Decision Tree', \n",
    "                       'Naive Bayes',\n",
    "                       'kNN', \n",
    "                       'SVM',\n",
    "                       'StackingClassifier']):\n",
    "\n",
    "    scores = model_selection.cross_val_score(clf, X, y, \n",
    "                                              cv=10, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a54a3-30fc-477c-bf33-667cf56dbaff",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacb2c7-f1c7-4321-9da8-826076936c3d",
   "metadata": {},
   "source": [
    "The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing models in order to balance out their individual weaknesses.\n",
    "\n",
    "In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.\n",
    "\n",
    "E.g., if the prediction for a given sample is\n",
    "\n",
    "classifier 1 -> class 1\n",
    "\n",
    "classifier 2 -> class 1\n",
    "\n",
    "classifier 3 -> class 2\n",
    "\n",
    "the VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class label.\n",
    "\n",
    "In the cases of a tie, the VotingClassifier will select the class based on the ascending sort order. E.g., in the following scenario\n",
    "\n",
    "classifier 1 -> class 2\n",
    "\n",
    "classifier 2 -> class 1\n",
    "\n",
    "the class label 1 will be assigned to the sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0198415c-3256-4c28-9161-cafb568e25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "911e8abc-0214-4da8-99a2-3c23654e8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = GaussianNB()\n",
    "clf3 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf4 = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afacf3c8-628f-4d5c-9591-6ff7a78c3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(\n",
    "     estimators=[('dt', clf1), ('nb', clf2), ('kNN', clf3), ('svm', clf4)],\n",
    "     voting='hard') # 'soft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd68c21c-760b-459a-92c5-f0705d7d9bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92 (+/- 0.06) [dt]\n",
      "Accuracy: 0.91 (+/- 0.05) [nb]\n",
      "Accuracy: 0.95 (+/- 0.05) [kNN]\n",
      "Accuracy: 0.95 (+/- 0.04) [svm]\n",
      "Accuracy: 0.94 (+/- 0.06) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "for clf, label in zip([clf1, clf2, clf3, clf4, eclf], ['dt', 'nb', 'kNN', 'svm', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c71470-7668-4e5e-ab52-92aa02b49962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
